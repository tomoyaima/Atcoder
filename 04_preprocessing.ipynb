{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "04_preprocessing.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tomoyaima/Atcoder/blob/master/04_preprocessing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EWZNs1z5iaPo",
        "colab_type": "text"
      },
      "source": [
        "# Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NeLBfkIyZmqE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%tensorflow_version 2.x"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C-7zw9lBaczu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "outputId": "d5eeba74-d0b5-4354-e286-92358494f61c"
      },
      "source": [
        "!pip install janome beautifulsoup4"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting janome\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/79/f0/bd7f90806132d7d9d642d418bdc3e870cfdff5947254ea3cab27480983a7/Janome-0.3.10-py2.py3-none-any.whl (21.5MB)\n",
            "\u001b[K     |████████████████████████████████| 21.5MB 1.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.6/dist-packages (4.6.3)\n",
            "Installing collected packages: janome\n",
            "Successfully installed janome-0.3.10\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g4G926luauIk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "55e82a39-2e06-4816-a14e-ad3afec814b2"
      },
      "source": [
        "!wget https://s3.amazonaws.com/amazon-reviews-pds/tsv/amazon_reviews_multilingual_JP_v1_00.tsv.gz -P data/\n",
        "!gunzip -d data/amazon_reviews_multilingual_JP_v1_00.tsv.gz"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-08-14 07:48:25--  https://s3.amazonaws.com/amazon-reviews-pds/tsv/amazon_reviews_multilingual_JP_v1_00.tsv.gz\n",
            "Resolving s3.amazonaws.com (s3.amazonaws.com)... 52.216.245.190\n",
            "Connecting to s3.amazonaws.com (s3.amazonaws.com)|52.216.245.190|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 94688992 (90M) [application/x-gzip]\n",
            "Saving to: ‘data/amazon_reviews_multilingual_JP_v1_00.tsv.gz’\n",
            "\n",
            "amazon_reviews_mult 100%[===================>]  90.30M  58.2MB/s    in 1.6s    \n",
            "\n",
            "2020-08-14 07:48:27 (58.2 MB/s) - ‘data/amazon_reviews_multilingual_JP_v1_00.tsv.gz’ saved [94688992/94688992]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BUpQZMDIigHe",
        "colab_type": "text"
      },
      "source": [
        "# utils.py"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "11lKYS5SbIJh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import string\n",
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "\n",
        "def filter_by_ascii_rate(text, threshold=0.9):\n",
        "    ascii_letters = set(string.printable)\n",
        "    rate = sum(c in ascii_letters for c in text) / len(text)\n",
        "    return rate <= threshold\n",
        "\n",
        "\n",
        "def load_dataset(filename, n=5000, state=6):\n",
        "    df = pd.read_csv(filename, sep='\\t')\n",
        "\n",
        "    # extracts Japanese texts.\n",
        "    is_jp = df.review_body.apply(filter_by_ascii_rate)\n",
        "    df = df[is_jp]\n",
        "\n",
        "    # sampling.\n",
        "    df = df.sample(frac=1, random_state=state)  # shuffle\n",
        "    grouped = df.groupby('star_rating')\n",
        "    df = grouped.head(n=n)\n",
        "    return df.review_body.values, df.star_rating.values\n",
        "\n",
        "\n",
        "def train_and_eval(x_train, y_train, x_test, y_test,\n",
        "                   lowercase=False, tokenize=None, preprocessor=None):\n",
        "    vectorizer = CountVectorizer(lowercase=lowercase,\n",
        "                                 tokenizer=tokenize,\n",
        "                                 preprocessor=preprocessor)\n",
        "    x_train_vec = vectorizer.fit_transform(x_train)\n",
        "    x_test_vec = vectorizer.transform(x_test)\n",
        "    clf = LogisticRegression(solver='liblinear')\n",
        "    clf.fit(x_train_vec, y_train)\n",
        "    y_pred = clf.predict(x_test_vec)\n",
        "    score = accuracy_score(y_test, y_pred)\n",
        "    print('{:.4f}'.format(score))\n"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ns5xpk4SikGx",
        "colab_type": "text"
      },
      "source": [
        "# preprocessing.py"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bqEw1k7SimQ7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "Preprocessings.\n",
        "\"\"\"\n",
        "import re\n",
        "\n",
        "from bs4 import BeautifulSoup\n",
        "from janome.tokenizer import Tokenizer\n",
        "t = Tokenizer()\n",
        "\n",
        "\n",
        "def clean_html(html, strip=False):\n",
        "    soup = BeautifulSoup(html, 'html.parser')\n",
        "    text = soup.get_text(strip=strip)\n",
        "    return text\n",
        "\n",
        "\n",
        "def tokenize(text):\n",
        "    return t.tokenize(text, wakati=True)\n",
        "\n",
        "\n",
        "def tokenize_base_form(text):\n",
        "    tokens = [token.base_form for token in t.tokenize(text)]\n",
        "    return tokens\n",
        "\n",
        "\n",
        "def normalize_number(text, reduce=False):\n",
        "    if reduce:\n",
        "        normalized_text = re.sub(r'\\d+', '0', text)\n",
        "    else:\n",
        "        normalized_text = re.sub(r'\\d', '0', text)\n",
        "    return normalized_text\n",
        "\n",
        "\n",
        "def truncate(sequence, maxlen):\n",
        "    return sequence[:maxlen]\n",
        "\n",
        "\n",
        "def remove_url(html):\n",
        "    soup = BeautifulSoup(html, 'html.parser')\n",
        "    for a in soup.findAll('a'):\n",
        "        a.replaceWithChildren()\n",
        "    return str(soup)\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BxemjNvwioHp",
        "colab_type": "text"
      },
      "source": [
        "# train.py"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UY6gowNWaPn-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        },
        "outputId": "eee640c3-de6d-4dbe-8c61-06450574e124"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "def main():\n",
        "    x, y = load_dataset('data/amazon_reviews_multilingual_JP_v1_00.tsv', n=1000)\n",
        "\n",
        "    x_train, x_test, y_train, y_test = train_test_split(x, y,\n",
        "                                                        test_size=0.2,\n",
        "                                                        random_state=42)\n",
        "    #単語分割のみ\n",
        "    print('Tokenization only.')\n",
        "    train_and_eval(x_train, y_train, x_test, y_test, tokenize=tokenize)\n",
        "    #単語の分割+クリーニング\n",
        "    print('Clean html.')\n",
        "    train_and_eval(x_train, y_train, x_test, y_test, tokenize=tokenize, preprocessor=clean_html)\n",
        "    #単語の分割+数字の正規化\n",
        "    print('Normalize number.')\n",
        "    train_and_eval(x_train, y_train, x_test, y_test, tokenize=tokenize, preprocessor=normalize_number)\n",
        "\n",
        "    print('Base form.')\n",
        "    train_and_eval(x_train, y_train, x_test, y_test, tokenize=tokenize_base_form)\n",
        "\n",
        "    print('Lower text.')\n",
        "    train_and_eval(x_train, y_train, x_test, y_test, tokenize=tokenize, lowercase=True)\n",
        "\n",
        "    print('Base form & Clean')\n",
        "    train_and_eval(x_train, y_train, x_test, y_test, tokenize=tokenize_base_form, preprocessor=clean_html)\n",
        "\n",
        "    print('Base form & Normalize number ')\n",
        "    train_and_eval(x_train, y_train, x_test, y_test, tokenize=tokenize_base_form, preprocessor=normalize_number)\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()\n"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tokenization only.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  \"the number of iterations.\", ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "0.4020\n",
            "Clean html.\n",
            "0.4090\n",
            "Normalize number.\n",
            "0.3940\n",
            "Base form.\n",
            "0.3930\n",
            "Lower text.\n",
            "0.3980\n",
            "Base form & Clean\n",
            "0.3940\n",
            "Base form & Normalize number \n",
            "0.3930\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Elgi8oLAbaBo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}